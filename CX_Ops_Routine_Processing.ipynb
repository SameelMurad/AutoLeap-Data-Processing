{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing Libraries**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aul7ODHBg6m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9mG___F2g0Jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gong Processing**"
      ],
      "metadata": {
        "id": "_bC16Ev37djJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_gong_data(input_file, output_file, hosts):\n",
        "    # Define the columns to extract from the data\n",
        "    gong_columns = [\n",
        "        'Gong Internal Company ID', 'Gong Internal Call ID', 'Gong Internal Owner ID', 'Call Title',\n",
        "        'Recorded Date', 'Recorded Year', 'Recorded Month', 'Recorded Day of Month', 'Recorded Week',\n",
        "        'Status', 'Conferencing Provider', 'Internal Meeting', 'Direction', 'Disposition', 'Duration (Sec.)',\n",
        "        'Word Count', 'Host', 'Host Email Address', 'Host Title', 'Manager 1 Name', 'Manager 1 Email',\n",
        "        'Manager 2 Name', 'Manager 2 Email', 'Manager 3 Name', 'Manager 3 Email', 'Manager 4 Name',\n",
        "        'Manager 4 Email', 'Manager 5 Name', 'Manager 5 Email', 'Manager 6 Name', 'Manager 6 Email',\n",
        "        'Manager 7 Name', 'Manager 7 Email', 'Manager 8 Name', 'Manager 8 Email', 'Manager 9 Name',\n",
        "        'Manager 9 Email', 'Manager 10 Name', 'Manager 10 Email', 'CRM - Account 1 Name', 'CRM - Lead 1 Name'\n",
        "    ]\n",
        "\n",
        "    # Read the CSV file and select the relevant columns\n",
        "    df = pd.read_csv(input_file, usecols=gong_columns)\n",
        "\n",
        "    # Filter the data based on the specified hosts\n",
        "    filtered_df = df[df['Host'].isin(hosts)]\n",
        "\n",
        "    # Export the filtered DataFrame to CSV\n",
        "    filtered_df.to_csv(output_file, sep=',', encoding='utf-8', index=False)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Specify the input and output file paths\n",
        "    input_file = '/content/Extensive_Call_Data_3799966398572700455_Jun-14-2023_06-33-26_4669043425477227646.csv'\n",
        "    output_file = 'Gong_Raw_Processed.csv'\n",
        "\n",
        "    # Specify the hosts to filter the data\n",
        "    hosts = [\n",
        "        'Afan Haque', 'AJ Patwa', 'Chris Matthews', 'Christian Braswell', 'Cisilia Chan', 'Colin Chan', 'Emma Ewing',\n",
        "        'Haaris Kirmani', 'Joel Cummings', 'Joel Farnsworth', 'Julia Blalock', 'Julia Pineda', 'Kathleen Hiemstra',\n",
        "        'Stephanie Gamble', 'Talia Bond', 'Sibel Gilani', 'Ahmed Raza Chohan', 'Marco Rios', 'Jordan Broyles',\n",
        "        'Katie Shank', 'Raahym Moeen', 'Ashraf Akhtar', 'Laraib Amjad', 'Anna Walsh', \"Morena D'Alma\"\n",
        "    ]\n",
        "\n",
        "    # Filter the Gong data based on hosts\n",
        "    filter_gong_data(input_file, output_file, hosts)\n",
        "\n",
        "    print(f\"Filtered data exported to: {output_file}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ej-D5JRp7gqQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "593977ec-92bd-4fc0-c0ba-653ae195a353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered data exported to: Gong_Raw_Processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Intercom Processing**"
      ],
      "metadata": {
        "id": "pXKGg1h78NGm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXkrJOwnC6fD",
        "outputId": "a2c8dc0f-0b8b-4a56-ef8b-bd780ed3003f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data processed and saved to: Intercom Processed.csv\n"
          ]
        }
      ],
      "source": [
        "# Adding the relevant columns we need from the data\n",
        "input_file = '/content/inbox-data-export.a540xj2u.a0773e50-68f7-4f77-9859-b7b8b3604b71.csv'\n",
        "output_file = 'Intercom Processed.csv'\n",
        "start_from = 119703\n",
        "\n",
        "# Defining a function to process the data\n",
        "def intercom_process(input_file, output_file, start_from=None):\n",
        "    intercom_columns = [\n",
        "        'Conversation ID', 'Conversation URL', 'Title', 'Conversation status',\n",
        "        'Conversation tags', 'Created at', 'Last updated at', 'Inbound/Outbound',\n",
        "        'Reopened', 'Closed', 'Conversation rating', 'Conversation rating requested',\n",
        "        'Conversation rating remark', 'Email', 'Location', 'Name',\n",
        "        'Assigned to (name)', 'Closed by (name)', 'Teammates participated',\n",
        "        'Time to first reply (seconds)', 'Time to last close (seconds)',\n",
        "        'Teammate replies', 'Channel'\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Read the CSV file and select only the specified columns\n",
        "        df_intercom = pd.read_csv(input_file, usecols=intercom_columns)\n",
        "\n",
        "        # Filter the dataframe based on the start_from value\n",
        "        if start_from is not None:\n",
        "            df_intercom = df_intercom[df_intercom['Conversation ID'] >= start_from]\n",
        "\n",
        "        # Sort the dataframe by column A in ascending order\n",
        "        df_intercom.sort_values(by='Conversation ID', ascending=True, inplace=True)\n",
        "\n",
        "        # Export the sorted dataframe to CSV\n",
        "        df_intercom.to_csv(output_file, sep=',', encoding='utf-8', index=False)\n",
        "\n",
        "        # Return the output file path\n",
        "        return output_file\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", str(e))\n",
        "        return None\n",
        "\n",
        "# Call the function on the relevant data\n",
        "processed_file = intercom_process(input_file, output_file, start_from)\n",
        "\n",
        "# Print the processed file path\n",
        "if processed_file:\n",
        "    print(\"Data processed and saved to:\", processed_file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Historical Customer Raw Processing**"
      ],
      "metadata": {
        "id": "LemscjTVUbJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_columns(csv_folder, predefined_columns):\n",
        "\n",
        "\n",
        "    # Iterate over all CSV files in the folder\n",
        "    for file in os.listdir(csv_folder):\n",
        "        if file.endswith('.csv'):\n",
        "            # Get the file path\n",
        "            csv_file = os.path.join(csv_folder, file)\n",
        "\n",
        "            # Read the CSV file\n",
        "            df = pd.read_csv(csv_file)\n",
        "\n",
        "            # Get the existing column names in the CSV file\n",
        "            existing_columns = df.columns.tolist()\n",
        "\n",
        "            # Identify the missing columns\n",
        "            missing_columns = list(set(predefined_columns) - set(existing_columns))\n",
        "\n",
        "            # Add the missing columns to the DataFrame\n",
        "            for column in missing_columns:\n",
        "                df[column] = None\n",
        "\n",
        "            # Reorder the columns according to the predefined column order\n",
        "            df = df[predefined_columns]\n",
        "\n",
        "            # Save the modified DataFrame back to the CSV file\n",
        "            df.to_csv(csv_file, index=False)\n",
        "\n",
        "\n",
        "def process_csv_files(csv_folder, reset_columns):\n",
        "    result_df = pd.DataFrame()\n",
        "\n",
        "    # Iterate over all CSV files in the folder\n",
        "    for file in os.listdir(csv_folder):\n",
        "        if file.endswith('.csv'):\n",
        "            # Get the file path\n",
        "            csv_file = os.path.join(csv_folder, file)\n",
        "\n",
        "            # Read the CSV file\n",
        "            df = pd.read_csv(csv_file)\n",
        "\n",
        "            # Apply data processing logic\n",
        "            processed_data = df.loc[df['Date'] == df['Date'].min()]\n",
        "\n",
        "            processed_data['Technician Hours logged against Services'] = processed_data['Technician Minutes logged against Services'] / 60\n",
        "            processed_data['Technician Hours clocked into workday'] = processed_data['Technician Minutes clocked into workday'] / 60\n",
        "            processed_data['Internal Review Request Sent'] = processed_data['Internal Review Requests Via Email'] + processed_data['Internal Review Requests Via SMS']\n",
        "            processed_data['Total payment Receipts Sent to customer'] = processed_data['Payment Receipts Sent Via Email'] + processed_data['Payment Receipts Sent Via SMS']\n",
        "            processed_data['Total Estimates sent to customers'] = processed_data['Estimates Sent Via Email'] + processed_data['Estimates Sent Via SMS']\n",
        "            processed_data['Total Invoices sent to customers'] = processed_data['Invoices Sent Via Email'] + processed_data['Invoices Sent Via SMS']\n",
        "            processed_data['Motor Parts + Labour items'] = processed_data['MOTOR Parts Added to Services'] + processed_data['MOTOR Labor Items Added to Services']\n",
        "\n",
        "            # Reset values to 0 if greater than 100 for specified columns\n",
        "            for column in reset_columns:\n",
        "                processed_data.loc[processed_data[column] > 100, column] = 0\n",
        "\n",
        "            # Append processed data to the result DataFrame\n",
        "            result_df = pd.concat([result_df, processed_data], ignore_index=True)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "\n",
        "# Set the folder path containing the CSV files\n",
        "csv_folder = '/content/drive/MyDrive/Your_CSV_Folder'\n",
        "\n",
        "# Specify the predefined column names\n",
        "predefined_columns = ['Date',\t'Salesforce ID',\t'Name',\t'Customers Created',\t'New Email Recorded',\t'Vehicle Created',\t'New ROs Created',\t'Services Created',\t'Line Items Created',\t'Revenue',\t'Payment Taken',\t'Appointment Created',\n",
        "               'Appointment Created with Customer associated',\t'Appointment Created with Vehicle associated',\t'Appointment Created with RO associated',\t'Appointment Reminders Sent',\t'Appointment Confirmations Received',\n",
        "               'Estimates Sent Via Email',\t'Estimates Sent Via SMS',\t'Estimates Approved',\t'Invoices Sent Via Email',\t'Invoices Sent Via SMS',\t'Payment Reciepts Sent Via Email',\t'Payment Reciepts Sent Via SMS',\n",
        "               'Internal Review Requests Via Email',\t'Internal Review Requests Via SMS',\t'Internal Reviews Received',\t'Internal Reviews at or above Minimum Rating Received',\t'Google Review Links Clicked',\n",
        "               'New Google Reviews Received',\t'Invoice Posted to QBO',\t'MOTOR Canned Services Added to ROs',\t'MOTOR Parts Added to Services',\t'MOTOR Labor Items Added to Services',\t'Vehicle Linked to MOTOR',\n",
        "               'Canned Service Added to ROs',\t'Inspection Checklists Associated to Services',\t'Technician Minutes clocked into workday',\t'Technician Minutes logged against Services',\t'Services Completed by Technicians',\n",
        "               'PartsTech Parts Orders Created',\t'PartsTech Parts Orders Submitted',\t'PartsTech Parts used on Services']\n",
        "\n",
        "# Specify the columns for resetting values to 0 if > 100\n",
        "reset_columns = [\n",
        "    'Customers Created',\n",
        "    'New Email Recorded',\n",
        "    'Vehicle Created',\n",
        "    'New ROs Created',\n",
        "    'Services Created',\n",
        "    'Line Items Created',\n",
        "]\n",
        "\n",
        "# Step 1: Match columns in all CSV files\n",
        "match_columns(csv_folder, predefined_columns)\n",
        "\n",
        "# Step 2: Process the CSV files and obtain consolidated data\n",
        "processed_data = process_csv_files(csv_folder, reset_columns)\n",
        "\n",
        "# Save the consolidated data to a CSV file\n",
        "processed_data.to_csv('/content/drive/MyDrive/Consolidated_Data.csv', index=False)\n"
      ],
      "metadata": {
        "id": "Xvbe0cWqUZv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Customer Raw Processing**\n"
      ],
      "metadata": {
        "id": "SHRkuVKNCqkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function with the updated input and output file names\n",
        "input_cust_raw = '/content/Alp-Health-Report-2023-06-04to2023-06-03.csv'\n",
        "output_cust_raw = 'Customer Raw Processed.csv'\n",
        "\n",
        "def process_customer_data(input_cust_raw, output_cust_raw):\n",
        "    # Define the columns needed from the data\n",
        "    raw_columns = [\n",
        "        'Date', 'Salesforce ID', 'Name', 'Customers Created', 'New Email Recorded',\n",
        "        'Vehicle Created', 'New ROs Created', 'Services Created', 'Line Items Created',\n",
        "        'Revenue', 'Payment Taken', 'Appointment Created',\n",
        "        'Appointment Created with Customer associated',\n",
        "        'Appointment Created with Vehicle associated',\n",
        "        'Appointment Created with RO associated', 'Appointment Reminders Sent',\n",
        "        'Appointment Confirmations Received', 'Estimates Sent Via Email',\n",
        "        'Estimates Sent Via SMS', 'Estimates Approved', 'Invoices Sent Via Email',\n",
        "        'Invoices Sent Via SMS', 'Payment Reciepts Sent Via Email',\n",
        "        'Payment Reciepts Sent Via SMS', 'Internal Review Requests Via Email',\n",
        "        'Internal Review Requests Via SMS', 'Internal Reviews Received',\n",
        "        'Internal Reviews at or above Minimum Rating Received',\n",
        "        'Google Review Links Clicked', 'New Google Reviews Received',\n",
        "        'Invoice Posted to QBO', 'MOTOR Canned Services Added to ROs',\n",
        "        'MOTOR Parts Added to Services', 'MOTOR Labor Items Added to Services',\n",
        "        'Vehicle Linked to MOTOR', 'Canned Service Added to ROs',\n",
        "        'Inspection Checklists Associated to Services',\n",
        "        'Technician Minutes clocked into workday',\n",
        "        'Technician Minutes logged against Services',\n",
        "        'Services Completed by Technicians', 'PartsTech Parts Orders Created',\n",
        "        'PartsTech Parts Orders Submitted', 'PartsTech Parts used on Services'\n",
        "    ]\n",
        "\n",
        "    # Read the CSV file and select only the specified columns\n",
        "    df = pd.read_csv(input_cust_raw, usecols=raw_columns)\n",
        "\n",
        "    # Filter the dataframe to get the row with the minimum date\n",
        "    min_date = df['Date'].min()\n",
        "    df_filtered = df[df['Date'] == min_date].copy()\n",
        "\n",
        "    # Calculate additional columns\n",
        "    df_filtered['Technician Hours logged against Services'] = df_filtered['Technician Minutes clocked into workday'] / 60\n",
        "    df_filtered['Technician Hours clocked into workday'] = df_filtered['Inspection Checklists Associated to Services'] / 60\n",
        "    df_filtered['Internal Review Request Sent'] = df_filtered['Internal Review Requests Via Email'] + df_filtered['Internal Review Requests Via SMS']\n",
        "    df_filtered['Total payment Reciepts Sent to customer'] = df_filtered['Payment Reciepts Sent Via Email'] + df_filtered['Payment Reciepts Sent Via SMS']\n",
        "    df_filtered['Total Estimates sent to customers'] = df_filtered['Estimates Sent Via Email'] + df_filtered['Estimates Sent Via SMS']\n",
        "    df_filtered['Total Invoices sent to customers'] = df_filtered['Invoices Sent Via Email'] + df_filtered['Invoices Sent Via SMS']\n",
        "    df_filtered['Motor Parts + Labour items'] = df_filtered['MOTOR Parts Added to Services'] + df_filtered['MOTOR Labor Items Added to Services']\n",
        "\n",
        "    # Export the processed dataframe to CSV\n",
        "    df_filtered.to_csv(output_cust_raw, sep=',', encoding='utf-8', index=False)\n",
        "\n",
        "    # Return the output file path\n",
        "    return output_cust_raw\n",
        "\n",
        "processed_file = process_customer_data(input_cust_raw, output_cust_raw)\n",
        "\n",
        "# Print the processed file path\n",
        "if processed_file:\n",
        "    print(\"Data processed and saved to:\", processed_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz-GFxSC-pdK",
        "outputId": "7808c51a-400f-420c-c9c9-49bb9d08eb70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data processed and saved to: Customer Raw Processed.csv\n"
          ]
        }
      ]
    }
  ]
}